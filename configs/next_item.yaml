# runner type tag
_type_: test.next_item
# wandb project
project: recsys.synth.mdp.next_item_test_scores
# wandb logging
log: True

seed: 42
cuda_device: ...

generation_phase:
  epochs: 1
  episodes_per_epoch: 3000
  samples_per_epoch: 10000
  use_cache: true

learning_phase:
  epochs: 250
  eval_schedule: 5
  eval_episodes: 20

env:
  global_config: ???
  seed: ???
  n_users: 100
  n_items: 1000
  max_episode_len: [50, 10]
  embeddings:
    n_dims: 8
    users: $_base.env.embeddings.10_clusters_for_8_dims
    items: $_base.env.embeddings.20_clusters_for_8_dims
  user_state:
    base_satiation: 0.15
    base_satiation_speed: [0.1, .4]
    satiation_drift: .2
    item_listening_trace_decay: .9
    item_repeat_penalty_power: .15
    early_stop_delta: 0.005
    similarity_metric: l2
    item_to_cluster_classification: softmax
    relevance_boosting: [0.2, 4.0]
    boosting_softness: [2.0, 3.0]
    discrete_actions:
      - [0.2, 0.08]
      - [0.37, 0.1]
      - [0.58, 0.03]
      - [0.75, 0.05]

observations:
  discrete: true
  history_size: 10

mdp:
  ratings_column: relevance_int
  framestack_size: 10
  reward_function_name: ones_reward #relevance_based_reward #"ones_reward"
  action_function_name: next_item_action
  episode_splitter_name: interaction_interruption #full_user_interaction #"interaction_interruption"

generation_model: ...
eval_model: ...

scoring:
  top_k: 10
  metrics: [ndcg, stat_hitrate, PC, ihitrate]
  tresh: [0.7, 0.5, 0.9]
  prediction_type: discrete

zoya_settings:
  top_k: 10
  ratings_column: relevance_int
  mdp_settings:
    framestack_size: 10
    reward_function_name: ones_reward #relevance_based_reward #"ones_reward"
    action_function_name: next_item_action
    episode_splitter_name: interaction_interruption #full_user_interaction #"interaction_interruption"

  scorer:
    metrics: [ndcg, stat_hitrate, PC, ihitrate]
    tresh: [0.7, 0.5, 0.9]
    prediction_type: discrete

  algo_settings:
    general_parameters:
      algo: DCQL
      batch_size: 1024
      use_gpu: False

    model_parameters:
      use_als: False
      emb_dim: 16
      hid_dim: 256
      memory_size: 10
      feature_size: 512
      #drr adrr
      state_repr_name: full_history
      freeze_emb: False
      attention_hidden_size: 32
      state_keys:
        - user
        - item
#        - score

model:
  _base_: models.discrete_random
  batch_size: 32

models:
  random:
    _type_: model.random
  discrete_random:
    _type_: model.discrete_random
  cql:
    _type_: model.cql
    _base_: default
    actor_learning_rate: 1e-3
    critic_learning_rate: 3e-3
    temp_learning_rate: 1e-4
    alpha_learning_rate: 1e-4
    alpha_threshold: 10.0
    conservative_weight: 5.0
    initial_temperature: 1.0
    initial_alpha: 1.0
    n_action_samples: 10
    soft_q_backup: False
  bc:
    _type_: model.bc
    _base_: default
    learning_rate: 5e-4
  discrete_bc:
    _type_: model.discrete_bc
    _base_: default
    learning_rate: 5e-4
  ddpg:
    _type_: model.ddpg
    _base_: default
    temp_learning_rate: 1e-4
    initial_temperature: 1.0
  discrete_cql:
    _type_: model.discrete_cql
    _base_: default
  sac:
    _type_: model.sac
    _base_: default
  sdac:
    _type_: model.sdac
    _base_: default
  discrete_sac:
    _type_: model.discrete_sac
    _base_: default
  default:
    actor_learning_rate: 1e-4
    critic_learning_rate: 3e-4
    temp_learning_rate: 1e-4  #?
    alpha_learning_rate: 1e-4 # ?
    q_func_factory: "mean"
    batch_size: 256
    n_frames: 1
    n_steps: 1
    gamma: 0.99
    tau: 0.005
    n_critics: 2
    initial_temperature: 1.0  #?
    initial_alpha: 1.0  #?
    alpha_threshold: 10.0 # ?
    conservative_weight: 5.0 # ?
    n_action_samples: 10  #?
    soft_q_backup: False  #?

wandb_init: $_base.wandb_init
cache:
  cache_root: cache
  enable: ???
  keep_last_n_experiments: 4
